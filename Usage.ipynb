{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9bbaa4",
   "metadata": {},
   "source": [
    "## **Usage and examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f09db7",
   "metadata": {},
   "source": [
    "Be sure to follow environment setup instructions provided in the README file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96615b48",
   "metadata": {},
   "source": [
    "### **Register available models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50182c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] lib.registry.ModelRegistry: Registered provider 'openai'\n",
      "[INFO] lib.registry.ModelRegistry: Registered provider 'anthropic'\n",
      "[INFO] lib.registry.ModelRegistry: Registered provider 'google'\n",
      "[INFO] lib.registry.ModelRegistry: Registered provider 'openrouter'\n"
     ]
    }
   ],
   "source": [
    "from lib.registry.setup_providers import register_all_models\n",
    "register_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f30f4e",
   "metadata": {},
   "source": [
    "You can inspect content of the registry in two ways:\n",
    "- list available providers\n",
    "- list available models under a specific provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9244593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.registry.ModelRegistry import ModelRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b01908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openai', 'anthropic', 'google', 'openrouter']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelRegistry.list_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1800f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google/gemini-2.5-flash-lite',\n",
       " 'google/gemini-2.5-pro',\n",
       " 'google/gemini-3-pro-preview',\n",
       " 'x-ai/grok-4-fast',\n",
       " 'x-ai/grok-4.1-fast',\n",
       " 'anthropic/claude-sonnet-4',\n",
       " 'minimax/minimax-m2',\n",
       " 'deepseek/deepseek-v3.2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelRegistry.list_models(\"openrouter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80dcd63",
   "metadata": {},
   "source": [
    "### **You can modify providers and models in a few ways**\n",
    "- Add new model under a specific provider\n",
    "- Remove a model from providers list\n",
    "- Add a new provider\n",
    "- Remove a provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c82e18",
   "metadata": {},
   "source": [
    "For model addition it is necessary to provide objects for the model and provider. This project utilizes the pydantic_ai library to do so. It's worth noting that many more providers are available at Pydantic AI's website and some models are compatible with OpenAI API. In case of pydantic_ai library usage, any additions are performed in the same way as the example shows, after replacing classes to the appropriate ones. For details see: https://ai.pydantic.dev/models/overview/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3d5b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] lib.registry.ModelRegistry: Model 'devstral-2512:free' added to provider 'openrouter'\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai.models.openrouter import OpenRouterModel\n",
    "from lib.providers.openrouter_config import OpenRouterConfig\n",
    "from lib.registry.ModelRegistry import ModelRegistry\n",
    "\n",
    "provider = OpenRouterConfig()._provider\n",
    "mistral = OpenRouterModel(model_name=\"mistralai/devstral-2512:free\", provider=provider)\n",
    "ModelRegistry.add_model(\"openrouter\", \"devstral-2512:free\", mistral) #devstral-2512:free is an alias for the registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51cd702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] lib.registry.ModelRegistry: Model 'o3-mini' removed from provider 'openai'\n"
     ]
    }
   ],
   "source": [
    "ModelRegistry.remove_model(\"openai\", \"o3-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab67b0",
   "metadata": {},
   "source": [
    "For provider addition follow these steps:\n",
    "- Add an API key for your new provider as an environmental variable (name: *_API_KEY). Reload kernel so it's added to the **keys** dictionary of the **ProviderKeys** class.\n",
    "- Add a new child class of the **BaseProviderConfig** class. If you are using pydantic_ai library, then the quickest way to add a provider is to copy an existing provider configuration class. After that, just replace the provider/model classes with appropriate ones found on Pydantic AI website and model names defined in the providers API.\n",
    "- Register your provider using your configuration class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.providers.anthropic_config import AnthropicConfig\n",
    "ModelRegistry.register_provider(AnthropicConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd849f18",
   "metadata": {},
   "source": [
    "Note: if you don't add an API key, the provider will be skipped. Same goes for all built-in providers. This means unused providers don't have to be deleted, unless there was a mistake in configuration of a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelRegistry.remove_provider(\"anthropic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5fb2f",
   "metadata": {},
   "source": [
    "### **Prompt builders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861a948",
   "metadata": {},
   "source": [
    "Before running a test you should check the dataset for column names and work out a way you want a prompt to be built. For example, QA tests need you to pass a question and a list of choices, both coming from seperate columns in dataset. There are a few prompt builders available for popular usage cases. \n",
    "\n",
    "Built-in prompt builders include:\n",
    "- **basic_prompt**: for cases where you just want to pass one column and ask for an answer based on it,\n",
    "- **provide_choices_prompt**: for cases where you want to attach a list of choices to your prompt (ex. QA tasks),\n",
    "- **classification_prompt**: for classification tasks with support for custom labels.\n",
    "\n",
    "Before running the test be sure to choose one of the above or define your own builder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d41ab",
   "metadata": {},
   "source": [
    "## **Run a benchmark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2126eb",
   "metadata": {},
   "source": [
    "Steps to running a benchmark: \n",
    "- using the **ModelSpec** class define pairs of (\"provider\", \"model\") that you want to test and put them into a list\n",
    "- using the **EvaluationSpec** class define which evaluation methods you want to use and put them into a list. For this you need to specify a name of evaluation method (to be used in the benchmark result dataset and report) and pass an evaluation function.\n",
    "- fill in all the **BenchmarkRunner** class parameters, including lists created in the latter steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138f967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.BenchmarkRunner import BenchmarkRunner, ModelSpec, EvaluationSpec\n",
    "from lib.evaluators.evaluate_accuracy import evaluate_accuracy\n",
    "from lib.prompt_builders.provide_choices_prompt import provide_choices_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb06fc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] lib.BenchmarkRunner: Running benchmark for model openrouter:devstral-2512:free\n",
      "[INFO] lib.TestRunner: Running test 'MMLU_anatomy' using provider=openrouter model=devstral-2512:free\n",
      "[INFO] lib.TestRunner: Loading dataset: hf://datasets/cais/mmlu/\n",
      "[INFO] lib.TestRunner: Loaded dataset with 135 rows\n",
      "[INFO] lib.TestRunner: Running concurrent generation (135 rows, max_concurrency=2)\n",
      "[INFO] lib.TestRunner: Results saved to: test_results\\MMLU_anatomy\\results_2026-01-23_16-31-37.jsonl\n",
      "[INFO] lib.TestRunner: Test 'MMLU_anatomy' completed successfully\n",
      "[INFO] lib.BenchmarkRunner: Evaluating openrouter:devstral-2512:free with accuracy\n",
      "[INFO] lib.BenchmarkRunner: Running benchmark for model openai:gpt-4o-mini\n",
      "[INFO] lib.TestRunner: Running test 'MMLU_anatomy' using provider=openai model=gpt-4o-mini\n",
      "[INFO] lib.TestRunner: Loading dataset: hf://datasets/cais/mmlu/\n",
      "[INFO] lib.TestRunner: Loaded dataset with 135 rows\n",
      "[INFO] lib.TestRunner: Running concurrent generation (135 rows, max_concurrency=2)\n",
      "[INFO] lib.TestRunner: Results saved to: test_results\\MMLU_anatomy\\results_2026-01-23_16-32-32.jsonl\n",
      "[INFO] lib.TestRunner: Test 'MMLU_anatomy' completed successfully\n",
      "[INFO] lib.BenchmarkRunner: Evaluating openai:gpt-4o-mini with accuracy\n",
      "[INFO] lib.BenchmarkRunner: Benchmark dataset saved to: benchmarks\\MMLU_anatomy\\dataset_2026-01-23_16-28-51.jsonl\n",
      "[INFO] lib.BenchmarkRunner: Benchmark report saved to: benchmarks\\MMLU_anatomy\\report_2026-01-23_16-28-51.json\n",
      "[INFO] lib.BenchmarkRunner: Benchmark completed successfully\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ModelSpec(\"openrouter\", \"devstral-2512:free\"),\n",
    "    ModelSpec(\"openai\", \"gpt-4o-mini\")\n",
    "]\n",
    "\n",
    "judge_prompt = \"Evaluate the quality of the provided summary compared to the reference.\"\n",
    "\n",
    "evaluations = [\n",
    "    EvaluationSpec(\n",
    "        name=\"accuracy\",\n",
    "        evaluator=lambda path: evaluate_accuracy(path)\n",
    "    )\n",
    "]\n",
    "\n",
    "splits = {'test': 'anatomy/test-00000-of-00001.parquet'}\n",
    "\n",
    "benchmark = BenchmarkRunner(\n",
    "    test_name=\"MMLU_anatomy\",\n",
    "    dataset_path=\"hf://datasets/cais/mmlu/\",\n",
    "    prompt_builder=lambda row: provide_choices_prompt(row),\n",
    "    system_prompt=\"You are taking an anatomy test. Provide the exact text of one of the choices as an answer. Do not include anything else in the answer aside from the correct choice text. Do not add any quotations and do not change letters case.\",\n",
    "    models=models,\n",
    "    evaluations=evaluations,\n",
    "    dataset_splits=splits['test'],\n",
    "    max_concurrency=2,\n",
    "    output_format=\"jsonl\",\n",
    "    report_format=\"json\"\n",
    ")\n",
    "\n",
    "results = await benchmark.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
